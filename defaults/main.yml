---
# Make sure vars from acromedia.aws-pci-network are also in your group_vars/all.yml.
# These include, but may not be limited to:
# - aws_resource_tag_slug
# - aws_resource_env_slug: Production
# - aws_resource_name_suffix: by Ansible
# - aws_vpc_region
# - aws_vpc_private_net_cidr
# - aws_vpc_public_subnets
# - aws_vpc_private_subnets
# - aws_billing_tags
# - aws_api_access_key
# - aws_api_secret_key

aws_resource_tag_slug: CHANGE_ME
aws_resource_env_slug: CHANGE_ME_TOO
aws_resource_name_suffix: by Ansible

ec2_ssh_pubkey_path: '~/.ssh/id_rsa.pub'  # Where on your local machine can the role find your RSA SSH public key?

route53_hosted_domain: subdomain.example.com    # This must already be set up and working.


# ##### UDF Template related ############
# ---- ec2_sysadmins_from_github --------
# Provide a list of github acccounts, plus the account name you want created on the server.
# Each entry will be created with passwordless sudo on the server, and their public keys given SSH access.
# Be careful with typos, obviously.
ec2_sysadmins_from_github: []
#  - github_account: im-batman
#    local_user: bwayne
#  - github_account: webs4life
#    local_user: pparker
# ---- ec2_sysadmins_from_gitlab --------
# Accepts just a flat list of names, as we assume private gitlab users share the same naming convention as sysadmin usernames.
# Be careful with typos, obviously.
ec2_sysadmins_from_gitlab: []
# - pparker
# - bbanner
ec2_sysadmins_from_gitlab_server_url: ''  # e.g: 'https://my-private-gitlab-server.example.com'
ec2_udf_template_path: "{{ role_path }}/templates/ec2-user-data-file.ubuntu-18.sh.j2"
ec2_post_provision_phone_home_command: '' # e.g:  curl -sS "ttps://mothership.example.com/?new_instance_id=${INSTANCE_ID}&ecdsa_fingerprint=${SSH_FINGERPRINT}&private_ip=${PRIVATE_IP}"


# Go look up the latest AMI for your region. AMI's are region specic.
ec2_defautlt_ami: ami-XXXXXXXXXXX   # E.g: ami-064efdb82ae15e93f (Ubuntu bionic server, release 20200131, ebs-ssd, amd64, ca-central-1, hvm)
ec2_default_volume_device_name: /dev/sda1
ec2_default_volume_type: gp3

# Bastion host
ec2_bastion_instance_type: t3.micro
ec2_bastion_ami: "{{ ec2_defautlt_ami }}"
ec2_bastion_volume_size_gb: 8
ec2_bastion_hostname_short: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-bastion"
ec2_bastion_fqdn: "{{ ec2_bastion_hostname_short }}.{{ route53_hosted_domain.lower() }}"
ec2_sg_rules_bastion:
  - proto: tcp
    from_port: 22
    to_port: 22
    cidr_ip: 0.0.0.0/0
ec2_bastion_iam_role_name: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-bastion"


# NFS (File server) node / cluster creation
nfs_provider: ec2 # Can be 'efs' or 'ec2'
# ec2_nfs_node_** vars only apply when nfs_provider == ec2
ec2_nfs_node_instance_type: t3.micro
ec2_nfs_node_ami: "{{ ec2_defautlt_ami }}"
ec2_nfs_node_volume_size_gb: 20
ec2_nfs_node_hostname_short: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-nfs"
ec2_nfs_node_fqdn: "{{ ec2_nfs_node_hostname_short }}"
ec2_nfs_node_iam_role_name: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-nfs-node"
# efs_** vars only apply when nfs_provider == efs
efs_encrypt: no
efs_performance_mode: general_purpose  # or max_io
efs_name: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-efs"
efs_throughput_mode: bursting
efs_wait: no  # do we wait until service is provisioned before continuing with the rest of the tasks?
efs_wait_timeout: 600 # in seconds
ec2_sg_rules_nfs_extra: []  # Default security group rules allow connections from other generated group IDs, not specific addresses, so we provide additions instead of overrides.
example_efs_access_points:
  - name: test
    root_dir_path: /test
    posix_uid: '0'   # If other than zero, EFS will ignore the client, and operate as this user.
    posix_gid: '0'   # If other than zero, EFS will ignore the client, and operate as this group.
    secondary_gids: '0'
    create_owner_uid: '1000'
    create_owner_gid: '1000'
    create_mode: '2775'
  - name: test2
    root_dir_path: /test2
    posix_uid: '0'   # If other than zero, EFS will ignore the client, and operate as this user.
    posix_gid: '0'   # If other than zero, EFS will ignore the client, and operate as this group.
    secondary_gids: '0'
    create_owner_uid: '1002'
    create_owner_gid: '1002'
    create_mode: '2775'
  - name: test3
    root_dir_path: /test3
    posix_uid: '0'   # If other than zero, EFS will ignore the client, and operate as this user.
    posix_gid: '0'   # If other than zero, EFS will ignore the client, and operate as this group.
    secondary_gids: '0'
    create_owner_uid: '2889'
    create_owner_gid: '889'
    create_mode: '2775'
efs_access_points: []   # "{{ example_efs_access_points }}" ...used for testing.

# App node creation
ec2_app_node_count_per_subnet: 1
ec2_app_node_instance_type: t3.micro
ec2_app_node_ami: "{{ ec2_defautlt_ami }}"
ec2_app_node_volume_size_gb: 20
ec2_app_node_hostname_short: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-app"
ec2_app_node_fqdn: "{{ ec2_app_node_hostname_short }}"
ec2_sg_rules_app_extra: []  # Default security group rules allow connections from other generated group IDs, not specific addresses, so we provide additions instead of overrides.
ec2_app_node_iam_role_name: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-app-node"

# ELK stack: Kibana front end, elastic search + log stash ... all in one for now.
ec2_elk_node: false
ec2_elk_node_instance_type: t3.micro
ec2_elk_node_ami: "{{ ec2_defautlt_ami }}"
ec2_elk_node_volume_size_gb: 20
ec2_elk_node_hostname_short: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-elk"
ec2_elk_node_fqdn: "{{ ec2_elk_node_hostname_short }}.{{ route53_hosted_domain.lower() }}"
ec2_sg_rules_elk_extra: []
ec2_elk_node_iam_role_name: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-elk-node"

# RDS instance creation
aws_database_provider: rds   # can be `rds` or `none`. RDS Requires at least 2 subnets.
rds_db_name: "{{ aws_resource_tag_slug.lower() }}{{ aws_resource_env_slug.lower() }}" # Cant contain punctuation.
rds_db_instance_identifier: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-db"
rds_engine: mariadb
rds_engine_version: '10.3'
rds_instance_class: db.t3.micro
rds_allocated_storage_gb: 20
rds_storage_type: gp2 #  or "io1" for provisioned iops.
rds_iops: 1000        #  ignored unless rds_storage_type == io1
rds_storage_encrypted: yes
rds_availability_zone: "{{ aws_vpc_region }}a"    # not a typo ...  - arbitrarily setting first zone, as in: ca-central-1a. Only used if rds_az_mode == single
rds_az_mode: single   # Can be "multi" or "single", as in "multi az" or "single az". If multi, then rds_availability_zone is ignored.
rds_backup_retention_days: 7
rds_wait: no      # The role almost always fails trying to read the instance data upon first creation. Its faster to just re-run the playbook than to wait for creation to fully complete.
rds_deletion_protection: no
ec2_sg_rules_db_extra: []  # Default security group rules allow connections from other generated group IDs, not specific addresses, so we provide additions instead of overrides.

# Elasticache creation
# See https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/WhatIs.Terms.html
elasticache_az_mode: single   # as in "multi" az or "single" az, or "none" to disable elasticache creation. If multi, then elasticache_availability_zone is ignored.
elasticache_cluster_name: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-cache"
elasticache_engine: redis
elasticache_engine_version: '5.0.6'
elasticache_port: 6379
elasticache_instance_type: cache.t3.micro
elasticache_num_nodes: 3   # only applies when elasticache_availability_mode == multi. If elasticache_availability_mode == single, then this is always treated as 1.
elasticache_state: present     # present / absent / rebooted
elasticache_availability_zone: "{{ aws_vpc_region }}a"    # not a typo ...  aka ca-central-1a
elasticache_wait: no   # The role almost always fails trying to read the instance data upon first creation. Its faster to just re-run the playbook than to wait for creation to fully complete.
ec2_sg_rules_cache_extra: []

# ELB creation
aws_load_balancer_provider: elb  # can be `elb` or `none`
elb_target_group_name: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-tgtGrp"
elb_target_group_port: 80
elb_target_group_protocol: http
elb_target_group_success_codes: "200,302"
elb_target_group_healthy_threshold_count: 5
elb_target_group_unhealthy_threshold_count: 2
elb_target_group_health_check_path: /
elb_listener_ssl_cert_arn: 'You_need_to_set_This_up_before_running_the_role' # e.g:  arn:aws:acm:us-east-1:123456789:certificate/a0a0a00-a00a0a0a0-a00a0a0a
elb_fqdn: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-lb.{{ route53_hosted_domain }}"
elb_dns_entry_overwrite: false
ec2_sg_rules_lb:
  - proto: tcp
    from_port: 80
    to_port: 80
    cidr_ip: 0.0.0.0/0
  - proto: tcp
    from_port: 443
    to_port: 443
    cidr_ip: 0.0.0.0/0
