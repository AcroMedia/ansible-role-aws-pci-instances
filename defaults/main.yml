---
# Make sure vars from acromedia.aws-pci-network are also in your group_vars/all.yml.
# These include, but may not be limited to:
# - aws_resource_tag_slug
# - aws_resource_env_slug: Production
# - aws_resource_name_suffix: by Ansible
# - aws_vpc_region
# - aws_vpc_private_net_cidr
# - aws_vpc_public_subnets
# - aws_vpc_private_subnets
# - aws_billing_tags
# - aws_api_access_key
# - aws_api_secret_key

aws_resource_tag_slug: CHANGE_ME
aws_resource_env_slug: CHANGE_ME_TOO
aws_resource_name_suffix: by Ansible

ec2_ssh_pubkey_path: '~/.ssh/id_rsa.pub'  # Where on your local machine can the role find your RSA SSH public key?

route53_hosted_domain: subdomain.example.com    # This must already be set up and working.


# ##### UDF Template related ############
# ---- ec2_sysadmins_from_github --------
# Provide a list of github acccounts, plus the account name you want created on the server.
# Each entry will be created with passwordless sudo on the server, and their public keys given SSH access.
# Be careful with typos, obviously.
ec2_sysadmins_from_github: []
#  - github_account: im-batman
#    local_user: bwayne
#  - github_account: webs4life
#    local_user: pparker
# ---- ec2_sysadmins_from_gitlab --------
# Accepts just a flat list of names, as we assume private gitlab users share the same naming convention as sysadmin usernames.
# Be careful with typos, obviously.
ec2_sysadmins_from_gitlab: []
# - pparker
# - bbanner
ec2_sysadmins_from_gitlab_server_url: ''  # e.g: 'https://my-private-gitlab-server.example.com'
ec2_udf_template_path: "{{ role_path }}/templates/ec2-user-data-file.ubuntu-18.sh.j2"
ec2_post_provision_phone_home_command: '' # e.g:  curl -sS "ttps://mothership.example.com/?new_instance_id=${INSTANCE_ID}&ecdsa_fingerprint=${SSH_FINGERPRINT}&private_ip=${PRIVATE_IP}"


# Go look up the latest AMI for your region. AMI's are region specic.
ec2_defautlt_ami: ami-XXXXXXXXXXX   # E.g: ami-064efdb82ae15e93f (Ubuntu bionic server, release 20200131, ebs-ssd, amd64, ca-central-1, hvm)
ec2_default_volume_device_name: /dev/sda1
ec2_default_volume_type: gp2

# Bastion host
ec2_bastion_instance_type: t3.micro
ec2_bastion_ami: "{{ ec2_defautlt_ami }}"
ec2_bastion_volume_size_gb: 8
ec2_bastion_hostname_short: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-bastion"
ec2_bastion_fqdn: "{{ ec2_bastion_hostname_short }}.{{ route53_hosted_domain.lower() }}"

# NFS (File server) node / cluster creation
nfs_provider: ec2 # Can be 'efs' or 'ec2'
# ec2_nfs_node_** vars only apply when nfs_provider == ec2
ec2_nfs_node_instance_type: t3.micro
ec2_nfs_node_ami: "{{ ec2_defautlt_ami }}"
ec2_nfs_node_volume_size_gb: 20
ec2_nfs_node_hostname_short: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-nfs"
ec2_nfs_node_fqdn: "{{ ec2_nfs_node_hostname_short }}"
# efs_** vars only apply when nfs_provider == efs
efs_encrypt: no
efs_performance_mode: general_purpose  # or max_io
efs_name: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-efs"
efs_throughput_mode: bursting
efs_wait: no  # do we wait until service is provisioned before continuing with the rest of the tasks?
efs_wait_timeout: 600 # in seconds

# App node creation
ec2_app_node_count_per_subnet: 1
ec2_app_node_instance_type: t3.micro
ec2_app_node_ami: "{{ ec2_defautlt_ami }}"
ec2_app_node_volume_size_gb: 20
ec2_app_node_hostname_short: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-app"
ec2_app_node_fqdn: "{{ ec2_app_node_hostname_short }}"

# ELK stack: Kibana front end, elastic search + log stash ... all in one for now.
ec2_elk_node: false
ec2_elk_node_instance_type: t3.micro
ec2_elk_node_ami: "{{ ec2_defautlt_ami }}"
ec2_elk_node_volume_size_gb: 20
ec2_elk_node_hostname_short: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-elk"
ec2_elk_node_fqdn: "{{ ec2_elk_node_hostname_short }}.{{ route53_hosted_domain.lower() }}"

# RDS instance creation
rds_db_name: "{{ aws_resource_tag_slug.lower() }}{{ aws_resource_env_slug.lower() }}" # Cant contain punctuation.
rds_db_instance_identifier: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-db-by-ansible"
rds_engine: mariadb
rds_engine_version: '10.3'
rds_instance_class: db.t3.micro
rds_allocated_storage_gb: 20
rds_storage_type: gp2 #  or "io1" for provisioned iops.
rds_iops: 1000        #  ignored unless rds_storage_type == io1
rds_storage_encrypted: yes
rds_availability_zone: "{{ aws_vpc_region }}a"    # not a typo ...  - arbitrarily setting first zone, as in: ca-central-1a
rds_multi_az: no
rds_backup_retention_days: 7
rds_wait: no
rds_deletion_protection: no

# Elasticache creation
elasticache_cluster_name: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-cache-by-ansible"
elasticache_engine: redis
elasticache_engine_version: '5.0.6'
elasticache_port: 6379
elasticache_instance_type: cache.t3.micro
elasticache_num_nodes: 1
elasticache_state: present     # present / absent / rebooted
elasticache_availability_zone: "{{ aws_vpc_region }}a"    # not a typo ...  aka ca-central-1a

# ELB creation
elb_listener_ssl_cert_arn: 'You_need_to_set_This_up_before_running_the_role' # e.g:  arn:aws:acm:us-east-1:123456789:certificate/a0a0a00-a00a0a0a0-a00a0a0a
elb_fqdn: "{{ aws_resource_tag_slug.lower() }}-{{ aws_resource_env_slug.lower() }}-lb.{{ route53_hosted_domain }}"
elb_dns_entry_overwrite: false
